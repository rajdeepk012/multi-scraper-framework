# Decision Flowchart: Which Scraping Method to Use

A practical guide for choosing the right approach when scraping a new bank/NBFC branch locator.

---

## The 5-Minute Recon Process

Before writing any code, spend 5 minutes with the browser:

### Step 1: Open the Branch Locator Page
Navigate to the institution's "Branch Locator" or "Our Branches" page.

### Step 2: Open DevTools Network Tab
Press `F12` → Network tab → filter by `XHR/Fetch`.

### Step 3: Interact with the Page
Click "Search", select a state/city, or scroll through results. Watch what appears in the Network tab.

### Step 4: Analyze What You See

---

## Decision Tree

```
START: Open branch locator + DevTools Network tab
│
├── Do you see XHR/Fetch requests returning JSON?
│   ├── YES: Is it a single endpoint returning all branches?
│   │   ├── YES ──────────────────────> Method 2: Direct API (Simple)
│   │   │                               Example: IndusInd Bank
│   │   │                               One request, one JSON response.
│   │   │
│   │   └── NO: Multiple endpoints (states→branches→details)?
│   │       └── YES ──────────────────> Method 2: Direct API (Multi-tier)
│   │                                   Example: APAC Finance, ICICI HFC
│   │                                   Chain API calls together.
│   │
│   └── Do the API calls require session cookies/CSRF tokens?
│       ├── Can tokens be obtained programmatically?
│       │   └── YES ──────────────────> Method 5: Hybrid
│       │                               Example: Aavas (PHPSESSID + csrfToken)
│       │                               Manual cookie copy + requests.
│       │
│       └── Tokens generated by complex JS?
│           └── YES ──────────────────> Method 3: Playwright
│                                       Let the browser handle auth.
│
├── NO JSON APIs visible. Is the page server-rendered HTML?
│   │   (Right-click → View Source → is the data visible in the HTML?)
│   │
│   ├── YES: Data is in standard HTML elements (<table>, <div>, <li>)?
│   │   └── YES ──────────────────────> Method 1: BeautifulSoup
│   │                                   Example: CSL Finance, Nido/Edelweiss
│   │                                   Parse HTML with CSS selectors.
│   │
│   ├── YES: Data is embedded in <script> tags (Next.js, Nuxt.js)?
│   │   └── YES ──────────────────────> Method 1 or 5: BS4 + JSON parsing
│   │                                   Example: Shubham Housing, SK Finance
│   │                                   Regex to extract, json.loads() to parse.
│   │
│   └── NO: Page is blank until JavaScript runs?
│       │
│       ├── Simple JS rendering (data loads automatically)?
│       │   └── YES ──────────────────> Method 3: Playwright
│       │                               Example: Protium, Shivalik Bank
│       │                               Render page, then parse HTML.
│       │
│       └── Complex interactions needed (dropdowns, clicks, pagination)?
│           │
│           ├── Need to intercept AJAX responses?
│           │   └── YES ──────────────> Method 4: Selenium + CDP
│           │                           Example: ART Housing
│           │                           Use Chrome DevTools Protocol.
│           │
│           └── Cascading dropdowns / multi-step forms?
│               └── YES ──────────────> Method 4: Selenium
│                                       Example: Saraswat Bank
│                                       Automate full interaction flow.
```

---

## Method Comparison Matrix

| Criteria | BS4 | Direct API | Playwright | Selenium | Hybrid |
|----------|-----|-----------|------------|----------|--------|
| **Speed** | Fast | Fast | Medium | Slow | Varies |
| **Setup complexity** | Low | Low | Medium | Medium | High |
| **Maintenance burden** | Low | Very Low | Medium | High | High |
| **Handles JS rendering** | No | N/A | Yes | Yes | Partial |
| **Handles authentication** | Manual | Manual | Auto | Auto | Manual |
| **Data quality** | Good | Excellent | Good | Good | Good |
| **Lines of code** | 30-80 | 30-100 | 50-120 | 80-200 | 60-150 |

---

## Common Gotchas

### 1. "The page looks static but data isn't in the HTML"
The site might be a Next.js/Nuxt.js app that embeds data as JSON in `<script>` tags. Check for `self.__next_f.push()` or `__NEXT_DATA__` in the page source.

**Solution**: Use Method 1 (BS4) with regex to extract the JSON payload.

### 2. "API calls work in the browser but fail in Python"
Usually a missing header. Common culprits:
- `X-Requested-With: XMLHttpRequest`
- `Referer: https://the-site.com/branch-locator`
- Session cookies (XSRF-TOKEN, PHPSESSID)

**Solution**: Copy ALL headers from the browser's Network tab, then remove them one by one to find the minimum required set.

### 3. "The site has anti-bot protection"
Signs: 403 errors, CAPTCHA pages, Cloudflare challenges.

**Solution**: Try Playwright with stealth mode first. If that fails, reduce request rate and rotate user agents.

### 4. "Data loads in chunks as you scroll"
Infinite scroll or lazy loading pattern.

**Solution**: Check Network tab for the pagination API. Usually `?page=N` or `?offset=N`. Use Direct API method.

### 5. "Dropdown selections trigger new data"
Common in bank branch locators: State → District → Branch cascading dropdowns.

**Solution**: Use Selenium with `Select()` class. Build nested loops: for each state, for each district, select and extract.

---

## Pro Tips

1. **Always start with the simplest method**. Try a plain `requests.get()` first. Upgrade to browser automation only if needed.

2. **Save raw responses**. Write HTML/JSON to files during development. This lets you iterate on parsing without re-fetching.

3. **Look for hidden APIs first**. Even complex-looking sites often have clean JSON endpoints. The 5 minutes spent in DevTools can save hours of Selenium code.

4. **Respect rate limits**. Add `time.sleep(0.5-2)` between requests. Getting blocked means starting over.

5. **Version your scrapers**. Sites change their markup. Keep old versions (like TVS Credit v1→v3) so you can debug regressions.
